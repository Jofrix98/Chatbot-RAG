import chromadb
from sentence_transformers import SentenceTransformer
import torch
import markdown
from bs4 import BeautifulSoup

# Cargar modelo de embeddings (elige un modelo eficiente)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # R√°pido y preciso

# Crear una funci√≥n para generar embeddings sin OpenAI
def compute_embedding(text):
    return embedding_model.encode(text).tolist()

# Cliente con persistencia en Colab
chroma_client = chromadb.PersistentClient(path="/content/chromadb_data")

# Eliminar la colecci√≥n si ya existe
collection_name = "fib_web_data"

# Obtener la colecci√≥n (sin eliminarla completamente)
collection = chroma_client.get_or_create_collection(name=collection_name)

# Eliminar todos los documentos existentes en la colecci√≥n
all_ids = collection.get()["ids"]
if all_ids:
    collection.delete(ids=all_ids)
    print(f"‚úÖ Se eliminaron {len(all_ids)} documentos de la colecci√≥n.")
else:
    print("‚ö†Ô∏è  No hab√≠a documentos previos en la colecci√≥n.")



# üîπ Lista de archivos a procesar
file_paths = [
    "mdFiles/ca.md",
    "mdFiles/en.md",
    "mdFiles/es.md",
    "mdFiles/index.md"
]

# üîπ Configurar ChromaDB
chroma_client = chromadb.PersistentClient(path="/content/chromadb_data")
collection_name = "fib_web_data"

# üîπ Eliminar la colecci√≥n si ya existe para evitar duplicados
try:
    chroma_client.delete_collection(collection_name)
    print(f"‚úÖ Colecci√≥n '{collection_name}' eliminada antes de insertar nuevos datos.")
except:
    print(f"‚ö†Ô∏è La colecci√≥n '{collection_name}' no exist√≠a.")

# üîπ Crear una nueva colecci√≥n limpia
collection = chroma_client.get_or_create_collection(name=collection_name)

# üîπ Cargar modelo de embeddings local
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def compute_embedding(text):
    """Genera embeddings para el texto usando Sentence Transformers."""
    return embedding_model.encode(text).tolist()

def chunk_text(text, chunk_size=200, overlap=25):
    """
    Divide el texto en fragmentos de tama√±o 'chunk_size' con un solapamiento 'overlap'.
    Garantiza que el solapamiento sea efectivo incluso en los √∫ltimos fragmentos.
    """
    chunks = []
    start = 0
    num_chunks = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))  # Asegura que no exceda el tama√±o total
        chunk = text[start:end]

        # Evitar fragmentos vac√≠os en los √∫ltimos trozos del texto
        if chunk:
            chunks.append(chunk)
            num_chunks += 1

        start += chunk_size - overlap  # Desplazamiento con solapamiento

    print(f"‚úÖ N√∫mero de fragmentos generados: {num_chunks} para este archivo.")
    return chunks

def md_to_text(md_content):
    """Convierte Markdown a texto plano sin etiquetas HTML."""
    html = markdown.markdown(md_content)  # Convierte Markdown a HTML
    soup = BeautifulSoup(html, "html.parser")  # Elimina etiquetas HTML
    return soup.get_text()

# üîπ Procesar m√∫ltiples archivos
for file_path in file_paths:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            markdown_text = f.read()

        # Convertir Markdown a texto limpio
        plain_text = md_to_text(markdown_text)

        # Dividir en chunks con overlapping
        chunks = chunk_text(plain_text)

        print(f"üìÑ Procesando archivo: {file_path}")

        # Agregar los fragmentos con embeddings a ChromaDB
        for i, chunk in enumerate(chunks):
            collection.add(
                ids=[f"{file_path}_chunk_{i}"],  # ID √∫nico basado en el nombre del archivo
                documents=[chunk],
                metadatas=[{"chunk_id": i, "source_file": file_path}],
                embeddings=[compute_embedding(chunk)]
            )

        print(f"‚úÖ Archivo {file_path} procesado y guardado en ChromaDB.\n")

    except Exception as e:
        print(f"‚ö†Ô∏è Error procesando el archivo {file_path}: {e}")