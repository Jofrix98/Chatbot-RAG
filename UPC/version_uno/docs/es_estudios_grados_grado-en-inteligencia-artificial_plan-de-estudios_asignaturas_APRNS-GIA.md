## Usted está aquí

[Inicio](/es) » [Estudios](/es/estudios) » [Grados](/es/estudios/grados) »
[Grado en Inteligencia Artificial](/es/estudios/grados/grado-en-inteligencia-
artificial) » [Plan de estudios](/es/estudios/grados/grado-en-inteligencia-
artificial/plan-de-estudios) » [Asignaturas](/es/estudios/grados/grado-en-
inteligencia-artificial/plan-de-estudios/asignaturas) » Aprendizaje por
Refuerzo y No Supervisado

  * Horas semanales
  * Competencias
  * Objetivos
  * Contenidos
  * Actividades
  * Metodología docente
  * Método de evaluación
  * Bibliografía
  * Capacidades previas

**Créditos**

6

**Tipos**

Obligatoria

**Requisitos**

Esta asignatura no tiene requisitos, pero tiene capacidades previas

**Departamento**

CS

**Web**

<https://sites.google.com/upc.edu/aprns>

Este curso cubre dos áreas importantes del aprendizaje automático: el
aprendizaje no supervisado y el aprendizaje por refuerzo. El aprendizaje no
supervisado es un tipo de aprendizaje automático en el que el algoritmo
aprende patrones y estructuras a partir de datos no etiquetados, mientras que
el aprendizaje por refuerzo es un tipo de aprendizaje automático en el que el
algoritmo aprende a través de recompensas o castigos.  
  
El curso empezará con una introducción a los conceptos y algoritmos
fundamentales del aprendizaje no supervisado, como autocodificadores, redes
adversarias o de difusión. Después, el curso pasará al aprendizaje por
refuerzo, cubriendo conceptos como los procesos de decisión de Markov, el
Q-learning y los métodos de gradiente de política. El curso explorará también
las últimas investigaciones en estos campos, incluyendo el aprendizaje por
refuerzo profundo y el aprendizaje profundo no supervisado.  
  
Al final del curso, los estudiantes tendrán una sólida base en el aprendizaje
no supervisado y por refuerzo, y serán capaces de aplicar estas técnicas a
problemas del mundo real.

## Profesorado

### Responsable

  * **Javier Béjar Alonso ( )**
  * **Mario Martín Muñoz ( )**

## Horas semanales

**Teoría**

2

**Problemas**

0

**Laboratorio**

2

**Aprendizaje dirigido**

0

**Aprendizaje autónomo**

6

## Competencias

### Competencias Transversales

#### Transversales

  * **CT6 [Avaluable]** \- Aprendizaje autónomo. Detectar deficiencias en el propio conocimiento y superarlas mediante la reflexión crítica y la elección de la mejor actuación para ampliar dicho conocimiento. 

#### Básicas

  * **CB5** \- Que los estudiantes hayan desarrollado aquellas habilidades de aprendizaje necesarias para emprender estudios posteriores con un alto grado de autonomía 

### Competencias Técnicas

#### Específicas

  * **CE18** \- Adquirir y desarrollar técnicas de aprendizaje computacional y diseñar e implementar aplicaciones y sistemas que las utilicen, incluyendo las dedicadas a extracción automática de información y conocimiento a partir de grandes volúmenes de datos. 

### Competencias Técnicas Genéricas

#### Genéricas

  * **CG2** \- Utilizar los conocimientos fundamentales y metodologías de trabajo sólidas adquiridos durante los estudios para adaptarse a los nuevos escenarios tecnológicos del futuro. 
  * **CG4** \- Razonar, analizando la realidad y diseñando algoritmos y formulaciones que la modelen. Identificar problemas y construir soluciones algorítmicas o matemáticas válidas, eventualmente nuevas, integrando el conocimiento multidisciplinar necesario, valorando distintas alternativas con espíritu crítico, justificando las decisiones tomadas, interpretando y sintetizando los resultados en el contexto del dominio de aplicación y estableciendo generalizaciones metodológicas a partir de aplicaciones concretas. 

## Objetivos

  1. Conocer qué tipo de problemas se pueden modelizar como un problema de aprendizaje por refuerzo e identificar las técnicas que se pueden aplicar para resolverlas   
Competencias relacionadas: CT6, CE18, CG2,

  2. Comprender la necesidad, los fundamentos y las particularidades del aprendizaje conductual y las diferencias que tiene con el aprendizaje automático supervisado y no supervisado.   
Competencias relacionadas: CE18, CG2,

  3. Comprender los algoritmos más importantes y el estado del arte en el área del aprendizaje por refuerzo.   
Competencias relacionadas: CE18, CG4,

  4. Saber formalizar computacionalmente un problema del mundo real como aprendizaje para reforzamiento y saber implementar en los entornos más actuales los algoritmos de aprendizaje que los resuelve   
Competencias relacionadas: CT6, CE18, CG2, CG4,

  5. Conocer los problemas que se pueden modelizar con algoritmos no supervisados produndos   
Competencias relacionadas: CT6, CE18, CG2,

  6. Entender las particularidades de los algoritmos no supervisados profundos   
Competencias relacionadas: CT6, CE18, CG4,

  7. Conocer los algoritmos más importantes y el estado del arte del aprendizaje no supervisado profundo   
Competencias relacionadas: CB5, CT6, CE18, CG2,

  8. Saber implementar y aplicar a un problema algoritmos de aprendizaje profundo utilizando el entorno más actual   
Competencias relacionadas: CB5, CT6, CE18, CG2,

## Contenidos

  1. Introducción: Aprendizaje del comportamiento en agentes y descripción de los principales elementos en el aprendizaje de refuerzo   
Intuición, motivación y definición del marco del aprendizaje pro refuerzo
(RL). Elementos clave en RL.

  2. Encontrando políticas óptimas mediante la programación dinámica   
Como aprender un comportamiento con conocimiento completo del modelo del
mundo: solución algebraica, evaluación iterada de políticas y evaluación
iterada de valores.

  3. Introducción a los enfoques sin modelos del mundo.   
Algoritmos básicos para el aprendizaje por refuerzo: Monte-Carlo, Q-learning,
Sarsa, TD (lambda). La necesidad de exploración. Diferencias entre los métodos
On-policy y Off-policy

  4. Aproximación de funciones en el aprendizaje por refuerzo   
La necesidad de la aproximación de funciones y métodos incrementales en RL. El
enfoque de Gradient Descent. RL con aproximación de función lineal. La tríada
mortal para la aproximación de funciones en RL. Métodos por lotes y redes
neuronales para la aproximación de funciones.

  5. Aprendizaje por refuerzo profundo (DRL)   
Introducción de DL en RL. Como tratar la mortal tríada con el algoritmo DQN.
Aplicación de DQN el caso de los juegos Atari. Evoluciones del algoritmo DQN:
Double DQN, Prioritized Experience Replay, aprendizaje en múltiples pasos y
funciones de valor distribuidas. Rainbow: el algoritmo de última generación
para un espacio de acción discreto.

  6. Métodos del gradiente en la política   
Qué hacer en espacios de acción continuos. Cómo las políticas probabilísticas
permiten aplicar el método de gradiente directamente en la red de políticas.
El algoritmo REINFORCE. Los algoritmos Actor-Critic. Algoritmos de última
generación en espacios de acción continuos: DDPG, TD3 y SAC.

  7. Temas avanzados: Como tratar el problema del refuerzo esparso   
El problema de la recompensa esparsa. Introducción a técnicas avanzadas de
exploración: curiosidad y empoderamiento en RL. Introducción al aprendizaje
curricular para facilitar el aprendizaje del objetivo. RL jerárquico para
aprender tareas complejas. El aprendizaje de las funciones de valor
universales y Hindsight Experience Replay (HER).

  8. Aprendizaje por refuerzo en el marco de múltiples agentes   
Aprendizaje de comportamientos en un entorno donde actúan varios agentes.
Aprendizaje de conductas cooperativas, Aprendizaje de conductas competitivas y
casos mixtos. Algoritmos de última generación. El caso especial de los juegos:
el caso AlfaGo y la extensión a Alfa-Zero.

  9. Introducción: El aprendizaje no supervisado profundo   
Introducción a la necesidad del aprendizaje no supervisado profundo y sus
aplicaciones

  10. Modelos autoregresivos   
Introducción al aprendizaje de distribuciones de probabilidad definidas como
distribuciones autorregresivas y principales modelos

  11. Flujos normalizantes   
Introducción a los flujos normalizados para el aprendizaje de distribuciones
de probabilidad

  12. Modelos de variables latentes   
Introducción a los modelos basados en variables latentes y a los
autocodificadores variacionales

  13. Redes adversarias Generativas   
Introducción a las redes adversarias generativas, generación condicionada y
sin condicionar, separación de atributos

  14. Redes de difusión   
Introducción a modelos basados en difusión de ruido, redes para eliminación de
ruido, condicionamiento, generación multimodal

  15. Aprendizaje por auto supervisión   
Introducción al aprendizaje por autosupervisión para el entrenamiento de redes
generadoras de características, metodos contrastivos y no contrastivos,
enmascaramiento

## Actividades

Actividad Acto evaluativo

  

### Introducción: Aprendizaje del comportamiento en agentes y descripción de
los principales elementos en el aprendizaje de refuerzo

  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Encontrando políticas óptimas mediante la programación dinámica

Como aprender un comportamiento con conocimiento completo del modelo del
mundo: solución algebraica, evaluación iterada de políticas y evaluación
iterada de valores.  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Introducción a los enfoques sin modelos. Monte-Carlo, Q-learning, Sarsa,
TD (lambda)

Desarrollo del tema correspondiente de la asignatura  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Aproximación de funciones en RL

  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Aprendizaje por refuerzo profundo

  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Métodos del gradiente en la política

Qué hacer en espacios de acción continuos. Cómo las políticas probabilísticas
permiten aplicar el método de gradiente directamente en la red de políticas.
El algoritmo REINFORCE. Los algoritmos Actor-Critic. Algoritmos de última
generación en espacios de acción continuos: DDPG, TD3 y SAC.  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Temas avanzados: Como tratar el problema del refuerzo esparso

El problema de la recompensa esparsa. Introducción a técnicas avanzadas de
exploración: curiosidad y empoderamiento en RL. Introducción al aprendizaje
curricular para facilitar el aprendizaje del objetivo. RL jerárquico para
aprender tareas complejas. El aprendizaje de las funciones de valor
universales y Hindsight Experience Replay (HER).  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Aprendizaje por refuerzo en el marco de múltiples agentes

Aprendizaje de comportamientos en un entorno donde actúan varios agentes.
Aprendizaje de conductas cooperativas, Aprendizaje de conductas competitivas y
casos mixtos. Algoritmos de última generación. El caso especial de los juegos:
el caso AlfaGo y la extensión a Alfa-Zero.  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

9h

  

### Control de la parte de aprendizaje por reforzamiento

  
**Objetivos:** 3 4 2 1  
**Semana:** 8 (Fuera de horario lectivo)  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

0h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

0h

  

### Introducción: El aprendizaje no supervisado profundo

Introducción a la necesidad del aprendizaje no supervisado profundo y sus
aplicaciones  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Modelos autoregresivos

Introducción al aprendizaje de distribuciones de probabilidad definidas como
distribuciones autorregresivas y principales modelos  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Flujos normalizantes

Introducción a los flujos normalizados para el aprendizaje de distribuciones
de probabilidad  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Modelos de variables latentes

Introducción a los modelos basados en variables latentes y a los
autocodificadores variacionales  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Redes adversarias Generativas

Introducción a las redes adversarias generativas, generación condicionada y
sin condicionar, separación de atributos  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

2h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

6h

  

### Redes de difusión y Aprendizaje por auto supervisión

Introducción a modelos basados en difusión de ruido, redes para eliminación de
ruido, condicionamiento, generación multimodal  
  

**Teoría**

2h

**Problemas**

0h

**Laboratorio**

4h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

9h

  

### Control del temario de aprendizaje no supervisado

  
**Objetivos:** 5 6 7 8  
**Semana:** 15 (Fuera de horario lectivo)  

**Teoría**

0h

**Problemas**

0h

**Laboratorio**

0h

**Aprendizaje dirigido**

0h

**Aprendizaje autónomo**

0h

  

## Metodología docente

Las clases están divididas en sesiones de teoría, problemas y laboratorio.  
  
En las sesiones de teoría se desarrollarán los conocimientos de la asignatura,
intercalando la exposición de nuevo material teórico con ejemplos y la
interacción con los alumnos para discutir los conceptos.  
  
En las clases de laboratorio se desarrollarán pequeñas prácticas utilizando
herramientas y utilizando librerías específicas que permitirán practicar y
reforzar los conocimientos de las clases de teoría.

## Método de evaluación

La asignatura comprenderá los siguientes actos evaluatorios:  
  
\- Informes de las actividades de laboratorio, que será necesario haber
entregado dentro de un plazo indicado para cada sesión (orientativamente, 2
semanas). A partir de una media ponderada de las notas de estos informes se
calculará una nota de laboratorio, L.  
  
\- Un primer examen parcial, realizado a mitad del curso, de la materia vista
hasta entonces. Sea P1 la nota obtenida en este examen.  
  
\- En el día designado dentro del período de exámenes, un segundo examen
parcial de la materia no cubierta por el primer parcial. Sea P2 la nota
obtenida en este examen.  
  
Las tres notas L, P1, P2 son entre 0 y 10.  
  
La nota final de la asignatura será:0.4*L +0.3*P1+ 0.3*P2  
  
Solo se pueden presentar a la reevaluación aquellas persones que, habiéndose
presentado al examen final lo hayan suspendido. La nota máxima que se puede
obtenier en la reevaluación es un 7.

## Bibliografía

### Básica:

  * **Reinforcement learning : an introduction** \- Sutton, Richard S; Barto, Andrew G, The MIT Press, [2020]. ISBN: 9780262039246   
[https://discovery.upc.edu/discovery/fulldisplay?docid=alma991004166329706711&context;=L&vid;=34CSUC_UPC:VU1⟨=ca](https://discovery.upc.edu/discovery/fulldisplay?docid=alma991004166329706711&context=L&vid=34CSUC_UPC:VU1&lang=ca)

  * **Grokking deep reinforcement learning** \- Morales, Miguel, Manning Publications, 2020. ISBN: 9781617295454   
[https://discovery.upc.edu/discovery/fulldisplay?docid=alma991004208939706711&context;=L&vid;=34CSUC_UPC:VU1⟨=ca](https://discovery.upc.edu/discovery/fulldisplay?docid=alma991004208939706711&context=L&vid=34CSUC_UPC:VU1&lang=ca)

  * **Generative deep learning: teaching machines to paint, write, compose, and play** \- Foster, D, O'Reilly Media, Incorporated, 2023. ISBN: 9781098134143   

  * **Hands-on image generation with TensorFlow: a practical guide to generating images and videos using deep learning** \- Cheong, S.Y, Packt Publishing, 2020. ISBN: 9781838821104   

### Complementaria:

  * **Deep reinforcement learning in action** \- Zai, Alexander; Brown, Brandon, Manning Publications Co , 2020\. ISBN: 9781617295430   
[https://discovery.upc.edu/discovery/fulldisplay?docid=alma991004203829706711&context;=L&vid;=34CSUC_UPC:VU1⟨=ca](https://discovery.upc.edu/discovery/fulldisplay?docid=alma991004203829706711&context=L&vid=34CSUC_UPC:VU1&lang=ca)

  * **Generative AI with Python and TensorFlow 2: harness the power of generative models to create images, text, and music** \- Babcock, J.; Bali, R, Packt Publishing , 2021\. ISBN: 9781800208506   

## Capacidades previas

Conocimientos basicos de Deep Learning y de Machine Learning.

## Dónde estamos

Edificio B6 del Campus Nord  
C/Jordi Girona Salgado,1-3  
08034 BARCELONA España  
Tel: (+34) 93 401 70 00

[informacio@fib.upc.edu](mailto:informacio@fib.upc.edu)

  * [__](/es/noticies/rss.rss)
  * [__](https://www.facebook.com/fib.upc)
  * [__](https://twitter.com/fib_upc)
  * [__](https://www.flickr.com/photos/fib-upc/albums)
  * [__](https://www.youtube.com/user/mediafib)
  * [__](https://www.instagram.com/fib.upc/)

[![](/sites/fib/files/images/banner-suport-
fib.jpg)](http://suport.fib.upc.edu)

## Contacta con la FIB

Su nombre *

Su dirección de correo electrónico *

Asunto *

Categoría * \- Por favor, elija -Aulas, equipos y servicios informáticosBuzon
de sugerenciasFelicitacionesInformación AcadémicaInformación de
movilidadInformación de los másteresInformación general de la FIBNoticias al
web de la FIBQuejas

Mensaje *

Leave this field blank

© Facultat d'Informàtica de Barcelona - Universitat Politècnica de Catalunya -
[Avíso legal sobre esta web](/es/aviso-legal-sobre-esta-web) \- Configuración
de privacidad

